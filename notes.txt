Things to do:

 - separate out concerns using free monads
 - add merging into the top-level, so we can do a combined validate/merge pipeline
 - progress/status reporting via scalajs
 - a http4s API

 ----

What distinct operations are going on in FileValidation?

 - parse the config
 - determine which files are missing / present
 - generate IDs for the files and job
 - record which files are missing
 - validate a file
 - persist row failures

There's a clear split here between database stuff and non-database stuff.

Obvious design glitches:
 - validateFile depends directly on io.file.readAll, should be EOTW
   - changed this so that validateFile is basically just a factory method, I think it's ok for it to to have io.file
     in now
 - initialiseFiles mixes concerns, it's doing DB stuff and file stuff
   - actually, is it just doing DB stuff?
   - split this into checkFilesExist / initialiseJob

----

Next up would be:
 - can I turn the function signatures in FVStreamOps, FVDbOps into algebra + interpreter?

Ok, done the basics of making composable algebras for stream and db, but I think there are other
bits in FileValidation that need turning into a DSL (i.e. parseRequiredFiles and checkFilesExist) before I can
really compose everything together.

----

Fundamental misunderstanding #1 - looks like you can only compose two interpreters
 - so, I think the thing to do here is to build FileValidation on top of StreamOps, and then combine
   the result with DatabaseOps at the top level
 - problem with that being that StreamOps is kind-of dependant on DatabaseOps, so it becomes circular

Nope, that's not a fundamental misunderstanding, that's SI-2712! Also, even with the type unification fix, ordering
of types is significant. Which can be a bit tricky.

Next thing to do is to work out which parts of the code are no longer relevant.

----

So, what if I wanted to change this so that two files are validated and
the 'good' streams resulting from that are merged?

I'd have to change validateFile so that it doesn't discard the 'good' data

There are quite a few streaming ops that could be composed better now...

- validate a row of data
- sink errors to the db
- grouping
- merging
- sink output to a file

I could have a set of operations that can be interpreted down to Stream...

----

In StreamThings, I've got a really clean implementation of validating and merging streams

Where to take it next?

 - [ ] Make the validation configurable (use a schema to specify the validation to apply to each input file)
 - [ ] Make the set of input streams configurable
 - [ ] Make the output transform configurable
 - [ ] Write the bad data for each input file to a separate output file
 - [x] Tidy up the codebase to get rid of failed experiments
 - [ ] DB logging
 - [ ] Run this on some actual data

This all assumes a mode of operation where someone puts together a set of input files in a directory, then
runs the code to produce a 'good' set of output files plus a set of discarded data.

----

So, let's say I had the following input files:

inbound/
  merge_a.csv
  merge_b.csv
  separate.csv

And some config that says:

[
  {
    "merge.csv": [
      {
        "name": "merge_a.csv",
        "columns": [
          {
            "name": "col_a",
            "max_width": 3,
            "required": true
          }, {
            "name": "col_b",
            "max_width": 5,
            "required": false
          }
        ]
      },
      {
        "name": "merge_b.csv",
        "columns": [
          {
            "name": "col_a",
            "max_width": 3,
            "required": true
          }, {
            "name": "col_b",
            "max_width": 5,
            "required": false
          }, {
            "name": "col_c",
            "max_width": 10,
            "required": true
          }
        ]
      }
    ]
  },
  {
    "separate.csv": [
      {
        "name": "separate.csv",
        "columns": [
           {
            "name": "col_a",
            "max_width": 3,
            "required": true
          }   
        ]
      }
    ]
  }
]


And I want to produce:

rejected/
  merge_a.csv
  merge_b.csv
  separate.csv

outbound/
  merge.csv
  separate.csv


What do I need to change to make that happen?

- [x] failure sink per inbound file
- [ ] multiple runs of validate and merge (perhaps without the merge)
- [ ] entry point
- [ ] parse the config (but do it hard-coded first?)

----

To write a spec for GroupKeys, what could I do?

 - generate an inbound stream with an arbitrary number of chunks, where each chunk has an arbitrary length
 - the outbound stream should have the same number of chunks

----

Next up, some profiling / benchmarking on MergeStreams maybe?

... adding a test to MergeStreamsSpec to look at chunkiness

Looks like the chunkiness is a bit better, still need to:

 - add scala check tests to make sure the implementation is correct
 - add benchmarks to compare old and new

From a very quick check, I appear to have made things slower :-)

What the!? Profile?

So, all the slowness was coming from the fact that the chunks going in to io.file.writeAllSync were tiny
(7 bytes, 1 byte, 7 bytes ...). The chunkiness is lost when converting records to strings and interspersing newlines.

Next steps...

 - Maybe see if I can preserve chunkiness through the output conversion? The chunkiness exists going in to it.
   If I can do that, then I can get rid of some 'rechunkN' calls.
 - In profiling, a couple of functions in MergeStreams looked slow, see what can be done to improve those

----

Can I benchmark the output transform?

Right. I think that's it then:

➜  large git:(group-keys-bench) ✗ java -jar fs2fv-assembly-0.1-SNAPSHOT.jar.norechunk .
Hit enter to begin

Inbound file records-a-2016-07-21.txt had 491 errors
Inbound file records-b-2016-07-21.txt had 491 errors
Took 8416 millis
Hit enter to end

➜  large git:(group-keys-bench) ✗ java -jar fs2fv-assembly-0.1-SNAPSHOT.jar.withrechunk .
Hit enter to begin

Inbound file records-a-2016-07-21.txt had 491 errors
Inbound file records-b-2016-07-21.txt had 491 errors
Took 2492 millis
Hit enter to end

➜  large git:(group-keys-bench) ✗ java -jar fs2fv-assembly-0.1-SNAPSHOT.jar.new .
Hit enter to begin

Inbound file records-a-2016-07-21.txt had 491 errors
Inbound file records-b-2016-07-21.txt had 491 errors
Took 1408 millis
Hit enter to end


Rechunking into 10k blocks before the write to disk gave a significant speedup (over 3x), and then preserving
chunkiness all the way through the pipe gave me another almost 2x.

Chunkiness is really important.

----

Where are the bottlenecks now?

 - lowestKey in MergeStreams seems to pop up a bit
   - it's got an implicit lookup in it, is that a problem?

 - the sampler reckons most of the CPU time was spent in createOutputChecked, start looking there
 - benchmark and profile?

Turns out removeRecordsWithKey was really slow. Switched to a mutable version and that part sped up 100x (according
to the benchmark). Reduced overall execution time on 50MB data by 4x.

If we can process 50MB in 15s, then we can do 200MB per min, 1GB in 5 min. Is that good enough?
