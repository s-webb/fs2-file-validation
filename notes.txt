Things to do:

 - separate out concerns using free monads
 - add merging into the top-level, so we can do a combined validate/merge pipeline
 - progress/status reporting via scalajs
 - a http4s API

 ----

What distinct operations are going on in FileValidation?

 - parse the config
 - determine which files are missing / present
 - generate IDs for the files and job
 - record which files are missing
 - validate a file
 - persist row failures

There's a clear split here between database stuff and non-database stuff.

Obvious design glitches:
 - validateFile depends directly on io.file.readAll, should be EOTW
   - changed this so that validateFile is basically just a factory method, I think it's ok for it to to have io.file
     in now
 - initialiseFiles mixes concerns, it's doing DB stuff and file stuff
   - actually, is it just doing DB stuff?
   - split this into checkFilesExist / initialiseJob

----

Next up would be:
 - can I turn the function signatures in FVStreamOps, FVDbOps into algebra + interpreter?

Ok, done the basics of making composable algebras for stream and db, but I think there are other
bits in FileValidation that need turning into a DSL (i.e. parseRequiredFiles and checkFilesExist) before I can
really compose everything together.

----

Fundamental misunderstanding #1 - looks like you can only compose two interpreters
 - so, I think the thing to do here is to build FileValidation on top of StreamOps, and then combine
   the result with DatabaseOps at the top level
 - problem with that being that StreamOps is kind-of dependant on DatabaseOps, so it becomes circular

Nope, that's not a fundamental misunderstanding, that's SI-2712! Also, even with the type unification fix, ordering
of types is significant. Which can be a bit tricky.

Next thing to do is to work out which parts of the code are no longer relevant.

----

So, what if I wanted to change this so that two files are validated and
the 'good' streams resulting from that are merged?

I'd have to change validateFile so that it doesn't discard the 'good' data

There are quite a few streaming ops that could be composed better now...

- validate a row of data
- sink errors to the db
- grouping
- merging
- sink output to a file

I could have a set of operations that can be interpreted down to Stream...

----

In StreamThings, I've got a really clean implementation of validating and merging streams

Where to take it next?

 - [ ] Make the validation configurable (use a schema to specify the validation to apply to each input file)
 - [ ] Make the set of input streams configurable
 - [ ] Make the output transform configurable
 - [ ] Write the bad data for each input file to a separate output file
 - [x] Tidy up the codebase to get rid of failed experiments
 - [ ] DB logging
 - [ ] Run this on some actual data

This all assumes a mode of operation where someone puts together a set of input files in a directory, then
runs the code to produce a 'good' set of output files plus a set of discarded data.

----

So, let's say I had the following input files:

inbound/
  merge_a.csv
  merge_b.csv
  separate.csv

And I want to produce:

rejected/
  merge_a.csv
  merge_b.csv
  separate.csv

outbound/
  merge.csv
  separate.csv


What do I need to change to make that happen?

- [x] failure sink per inbound file
- [ ] multiple runs of validate and merge (perhaps without the merge)
- [ ] entry point

----

To write a spec for GroupKeys, what could I do?

 - generate an inbound stream with an arbitrary number of chunks, where each chunk has an arbitrary length
 - the outbound stream should have the same number of chunks